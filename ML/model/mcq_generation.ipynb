{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40299ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (0.1.99)\n",
      "Requirement already satisfied: transformers in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (4.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ajitesh\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\n",
      "Successfully installed tokenizers-0.13.3\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install --upgrade transformers\n",
    "!pip install tokenizers==0.11.1\n",
    "!pip install yake\n",
    "!pip install sentence_transformers\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install pywsd\n",
    "!pip install find_sentances\n",
    "!pip install datasets\n",
    "!pip install flashtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3717ffd",
   "metadata": {},
   "source": [
    "# Summarization using t5(for big texts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14bd7632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AJITESH\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: The Nile is 4,160 miles long — the world’s longest river. the Nile is 4,160 miles long — the world’s longest river. The Nile is 4,160 miles long — the world’s longest river. The Nile is 4,160 miles long — the world’s longest river. The Nile is 4,160 miles long — the world’s longest river. The Nile is 4,160 miles long — the world’s longest river. The Nile is 4,160 miles long\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name ='t5-base'\n",
    "model =T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Input text for summarization\n",
    "# input_text = \"Elon Musk has shown again he can influence the digital currency market with just his tweets. After saying that his electric vehicle-making company Tesla will not accept payments in Bitcoin because of environmental concerns, he tweeted that he was working with developers of Dogecoin to improve system transaction efficiency. Following the two distinct statements from him, the world's largest cryptocurrency hit a two-month low, while Dogecoin rallied by about 20 percent. The SpaceX CEO has in recent months often tweeted in support of Dogecoin, but rarely for Bitcoin.  In a recent tweet, Musk put out a statement from Tesla that it was concerned about the rapidly increasing use of fossil fuels for Bitcoin (price in India) mining and transaction, and hence was suspending vehicle purchases using the cryptocurrency. A day later he again tweeted saying, To be clear, I strongly believe in crypto, but it can't drive a massive increase in fossil fuel use, especially coal. It triggered a downward spiral for Bitcoin value but the cryptocurrency has stabilised since.  A number of Twitter users welcomed Musk's statement. One of them said it's time people started realising that Dogecoin is here to stay and another referred to Musk's previous assertion that crypto could become the world's future currency.\"\n",
    "input_text=\"The Greek historian knew what he was talking about. The Nile River fed Egyptian civilization for hundreds of years. The Longest River the Nile is 4,160 miles long — the world’s longest river. It begins near the equator in Africa and flows north to the Mediterranean Sea\"\n",
    "# Tokenize input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate summary\n",
    "summary_ids = model.generate(input_ids, min_length=150,max_length=500, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary\n",
    "print(\"Generated Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def9464a",
   "metadata": {},
   "source": [
    "# LexRank Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b14df742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Greek historian knew what he was talking about. The Nile River fed Egyptian civilization for hundreds of years. The Longest River the Nile is 4,160 miles long — the world’s longest river. It begins near the equator in Africa and flows north to the Mediterranean Sea\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "def extractive_summarization(input_text, num_words=50):\n",
    "    # Initialize parser and tokenizer\n",
    "    parser = PlaintextParser.from_string(input_text, Tokenizer(\"english\"))\n",
    "\n",
    "    # Initialize LexRank summarizer\n",
    "    summarizer = LexRankSummarizer()\n",
    "    \n",
    "    # Generate extractive summary\n",
    "    sentences = summarizer(parser.document, num_words)\n",
    "    \n",
    "    # Accumulate sentences until the desired number of words is reached\n",
    "    summary_text = \"\"\n",
    "    word_count = 0\n",
    "    for sentence in sentences:\n",
    "        if word_count + len(sentence.words) <= num_words:\n",
    "            summary_text += \" \" + str(sentence)\n",
    "            word_count += len(sentence.words)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return summary_text.strip()\n",
    "\n",
    "# Set the desired number of words in the summary (default: 50)\n",
    "num_words = 150\n",
    "input_text = \"The Greek historian knew what he was talking about. The Nile River fed Egyptian civilization for hundreds of years. The Longest River the Nile is 4,160 miles long — the world’s longest river. It begins near the equator in Africa and flows north to the Mediterranean Sea\"\n",
    "\n",
    "# Perform extractive summarization on the input_text\n",
    "summary = extractive_summarization(input_text, num_words)\n",
    "\n",
    "# Print the extractive summary\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5e6bc",
   "metadata": {},
   "source": [
    "# Extractive summarization(maybe bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e2a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword extraction using bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9be52",
   "metadata": {},
   "source": [
    "# yake for keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "743d76c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tesla CEO Elon Musk has shown', 0.00023738346258411956)\n",
      "('Tesla CEO Elon Musk has shown again he can influence', 0.00024194389453223992)\n",
      "('Tesla CEO Elon Musk', 0.0005209326441347601)\n",
      "('Tesla CEO Elon', 0.0011681822716946644)\n",
      "('CEO Elon Musk has shown', 0.0014162249740638383)\n",
      "('CEO Elon Musk has shown again he can influence', 0.0015671936229382686)\n",
      "('influence the digital currency market with just his tweets', 0.002452914552832958)\n",
      "('CEO Elon Musk', 0.003067563617452901)\n",
      "('Elon Musk has shown again he can influence the digital', 0.0037997710233147013)\n",
      "('shown again he can influence the digital currency market', 0.006265315911975775)\n",
      "('CEO Elon', 0.007231742036985983)\n",
      "('influence the digital currency market', 0.007837201399372218)\n",
      "('digital currency market with just his tweets', 0.010114375219626902)\n",
      "('Tesla CEO', 0.013531236103144509)\n",
      "('Elon Musk has shown', 0.016227769259426296)\n",
      "('Musk has shown again he can influence the digital currency', 0.024202193949629327)\n",
      "('shown again he can influence the digital currency', 0.02636628547060855)\n",
      "('influence the digital currency', 0.031950003478514954)\n",
      "('digital currency market', 0.031950003478514954)\n",
      "('Elon Musk', 0.03488448113720666)\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "# YAKE (Yet Another Keyword Extractor): YAKE algorithm combines statistical and linguistic features to identify important keywords.\n",
    "# It takes into account features like word position, frequency, and context to extract keywords.\n",
    "\n",
    "# Specify the language and number of keywords to extract\n",
    "language = \"en\"\n",
    "n_keywords = 10\n",
    "\n",
    "# Initialize the YAKE keyword extractor\n",
    "kw_extractor = yake.KeywordExtractor(lan=language, n=n_keywords, dedupLim=0.9, dedupFunc='seqm')\n",
    "\n",
    "# Extract keywords from the summarized text\n",
    "keywords = kw_extractor.extract_keywords(summary)\n",
    "\n",
    "\n",
    "# Print the extracted keywords\n",
    "for keyword in keywords:\n",
    "    print(keyword)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007c377",
   "metadata": {},
   "source": [
    "# distilbert-base-nli-mean-tokens-keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0377f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['160', 'africa', 'begins', 'civilization', 'egyptian', 'equator', 'fed', 'flows', 'greek', 'historian', 'hundreds', 'knew', 'long', 'longest', 'mediterranean', 'miles', 'near', 'nile', 'north', 'river', 'sea', 'talking', 'world', 'years']\n",
      "\n",
      "['160', 'africa', 'begins', 'civilization', 'egyptian', 'equator', 'fed', 'flows', 'greek', 'historian', 'hundreds', 'knew', 'long', 'longest', 'mediterranean', 'miles', 'near', 'nile', 'north', 'river', 'sea', 'talking', 'world', 'years']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AJITESH\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_gram_range = (1, 1)   #only a single word(1 word) is extacted\n",
    "stop_words = \"english\" #which means common english words are not included\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count_summary = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([summary])\n",
    "count_paragraph = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([input_text])\n",
    "\n",
    "candidates_summary = count_summary.get_feature_names()\n",
    "candidates_paragraph = count_paragraph.get_feature_names()\n",
    "print(candidates_summary)\n",
    "print()\n",
    "print(candidates_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "271ef276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding_summary = model.encode([summary])\n",
    "candidate_embeddings_summary = model.encode(candidates_summary)\n",
    "\n",
    "doc_embedding_paragraph = model.encode([input_text])\n",
    "candidate_embeddings_paragraph = model.encode(candidates_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc8a1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 10\n",
    "distances = cosine_similarity(doc_embedding_summary, candidate_embeddings_summary)\n",
    "keywords_summary = [candidates_summary[index] for index in distances.argsort()[0][-top_n:]]\n",
    "keywords_paragraph = [candidates_paragraph[index] for index in distances.argsort()[0][-top_n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e7151b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['north', 'equator', 'hundreds', 'years', 'long', 'mediterranean', 'river', 'nile', 'longest', 'egyptian']\n",
      "['north', 'equator', 'hundreds', 'years', 'long', 'mediterranean', 'river', 'nile', 'longest', 'egyptian']\n"
     ]
    }
   ],
   "source": [
    "print(keywords_summary) #keyword extracted from the summarized text\n",
    "print(keywords_paragraph) #keywords extracted from the whole text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1569b7",
   "metadata": {},
   "source": [
    "# keyword extraction using TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20325eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['river', 'nile', 'longest', 'years', 'historian', 'africa', 'begins', 'civilization', 'egyptian', 'equator', 'fed', 'flows', 'greek', 'knew', 'hundreds', 'world', 'long', 'mediterranean', 'miles', 'near', 'north', 'sea', 'talking', '160']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=n_gram_range, stop_words=stop_words)\n",
    "\n",
    "# Fit the vectorizer on the summary text\n",
    "tfidf.fit([summary])\n",
    "\n",
    "# Transform the summary text to TF-IDF representation\n",
    "tfidf_summary = tfidf.transform([summary])\n",
    "\n",
    "# Extract feature names (keywords) sorted by their TF-IDF scores\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "tfidf_scores = tfidf_summary.toarray().flatten()\n",
    "\n",
    "# Sort keywords based on TF-IDF scores\n",
    "keywords_summary = [feature_names[idx] for idx in tfidf_scores.argsort()[::-1]]\n",
    "\n",
    "# Print the extracted keywords\n",
    "print(keywords_summary)   #keywords extracted from the summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98660c3",
   "metadata": {},
   "source": [
    "# keyword extraction using pke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb9fe471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('longest river', 0.15917107791635215), ('nile river', 0.14941014268825292), ('hundreds', 0.09953002380416344), ('egyptian civilization', 0.09698493757657037), ('years', 0.09516292892248306), ('world', 0.08363094298767532), ('miles', 0.08197327961293424), ('equator', 0.0751982325530555), ('africa', 0.0729123558883408), ('mediterranean sea', 0.046134243100310654)]\n"
     ]
    }
   ],
   "source": [
    "import pke\n",
    "\n",
    "# initialize keyphrase extraction model, here TopicRank\n",
    "extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "# load the content of the document, here document is expected to be a simple \n",
    "# test string and preprocessing is carried out using spacy\n",
    "extractor.load_document(input=input_text, language='en')\n",
    "\n",
    "# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
    "# and adjectives (i.e. `(Noun|Adj)*`)\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# N-best selection, keyphrases contains the 10 highest scored candidates as\n",
    "# (keyphrase, score) tuples\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "print(keyphrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e74f7f",
   "metadata": {},
   "source": [
    "# Sentence Mapping(Ramsri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdb2d9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'north': ['It begins near the equator in Africa and flows north to the Mediterranean Sea'], 'equator': ['It begins near the equator in Africa and flows north to the Mediterranean Sea'], 'hundreds': ['The Nile River fed Egyptian civilization for hundreds of years.'], 'years': ['The Nile River fed Egyptian civilization for hundreds of years.'], 'long': ['The Longest River the Nile is 4,160 miles long — the world’s longest river.'], 'mediterranean': ['It begins near the equator in Africa and flows north to the Mediterranean Sea'], 'river': ['The Longest River the Nile is 4,160 miles long — the world’s longest river.', 'The Longest River the Nile is 4,160 miles long — the world’s longest river.', 'The Nile River fed Egyptian civilization for hundreds of years.'], 'nile': ['The Longest River the Nile is 4,160 miles long — the world’s longest river.', 'The Nile River fed Egyptian civilization for hundreds of years.'], 'longest': ['The Longest River the Nile is 4,160 miles long — the world’s longest river.', 'The Longest River the Nile is 4,160 miles long — the world’s longest river.'], 'egyptian': ['The Nile River fed Egyptian civilization for hundreds of years.']}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    sentences = [sent_tokenize(text)]\n",
    "    sentences = [y for x in sentences for y in x]\n",
    "    # Remove any short sentences less than 20 letters.\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
    "    return sentences\n",
    "\n",
    "def get_sentences_for_keyword(keywords, sentences):\n",
    "    keyword_processor = KeywordProcessor()\n",
    "    keyword_sentences = {}\n",
    "    for word in keywords:\n",
    "        keyword_sentences[word] = []\n",
    "        keyword_processor.add_keyword(word)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        keywords_found = keyword_processor.extract_keywords(sentence)\n",
    "        for key in keywords_found:\n",
    "            keyword_sentences[key].append(sentence)\n",
    "    \n",
    "    for key in keyword_sentences.keys():\n",
    "        values = keyword_sentences[key]\n",
    "        values = sorted(values, key=len, reverse=True)\n",
    "        keyword_sentences[key] = values\n",
    "    \n",
    "    return keyword_sentences\n",
    "\n",
    "sentences = tokenize_sentences(input_text)\n",
    "keyword_sentence_mapping = get_sentences_for_keyword(keywords_summary, sentences)\n",
    "\n",
    "print(keyword_sentence_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7826471",
   "metadata": {},
   "source": [
    "# Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4d7708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the month of Following the two distinct statements from him, the world's largest cryptocurrency hit a two-month low, while Dogecoin rallied by about 20 percent.?\n",
      "What is the month of The SpaceX CEO has in recent months often tweeted in support of Dogecoin, but rarely for Bitcoin.  ?\n",
      "What is the developers of After saying that his electric vehicle-making company Tesla will not accept payments in Bitcoin because of environmental concerns, he tweeted that he was working with developers of Dogecoin to improve system transaction efficiency.?\n",
      "What is the crypto of Following the two distinct statements from him, the world's largest cryptocurrency hit a two-month low, while Dogecoin rallied by about 20 percent.?\n",
      "What is the crypto of In a recent tweet, Musk put out a statement from Tesla that it was concerned about the rapidly increasing use of fossil fuels for Bitcoin (price in India) mining and transaction, and hence was suspending vehicle purchases using the cryptocurrency.?\n",
      "What is the crypto of A day later he again tweeted saying, To be clear, I strongly believe in crypto, but it can't drive a massive increase in fossil fuel use, especially coal.?\n",
      "What is the crypto of It triggered a downward spiral for Bitcoin value but the cryptocurrency has stabilised since.  ?\n",
      "What is the crypto of One of them said it's time people started realising that Dogecoin is here to stay and another referred to Musk's previous assertion that crypto could become the world's future currency.?\n",
      "What is the cryptocurrency of Following the two distinct statements from him, the world's largest cryptocurrency hit a two-month low, while Dogecoin rallied by about 20 percent.?\n",
      "What is the cryptocurrency of In a recent tweet, Musk put out a statement from Tesla that it was concerned about the rapidly increasing use of fossil fuels for Bitcoin (price in India) mining and transaction, and hence was suspending vehicle purchases using the cryptocurrency.?\n",
      "What is the cryptocurrency of It triggered a downward spiral for Bitcoin value but the cryptocurrency has stabilised since.  ?\n",
      "What is the tesla of After saying that his electric vehicle-making company Tesla will not accept payments in Bitcoin because of environmental concerns, he tweeted that he was working with developers of Dogecoin to improve system transaction efficiency.?\n",
      "What is the tesla of In a recent tweet, Musk put out a statement from Tesla that it was concerned about the rapidly increasing use of fossil fuels for Bitcoin (price in India) mining and transaction, and hence was suspending vehicle purchases using the cryptocurrency.?\n",
      "What is the ceo of The SpaceX CEO has in recent months often tweeted in support of Dogecoin, but rarely for Bitcoin.  ?\n",
      "What is the tweeted of After saying that his electric vehicle-making company Tesla will not accept payments in Bitcoin because of environmental concerns, he tweeted that he was working with developers of Dogecoin to improve system transaction efficiency.?\n",
      "What is the tweeted of The SpaceX CEO has in recent months often tweeted in support of Dogecoin, but rarely for Bitcoin.  ?\n",
      "What is the tweeted of A day later he again tweeted saying, To be clear, I strongly believe in crypto, but it can't drive a massive increase in fossil fuel use, especially coal.?\n",
      "What is the tweets of Elon Musk has shown again he can influence the digital currency market with just his tweets.?\n",
      "What is the twitter of A number of Twitter users welcomed Musk's statement.?\n",
      "What is the bitcoin of After saying that his electric vehicle-making company Tesla will not accept payments in Bitcoin because of environmental concerns, he tweeted that he was working with developers of Dogecoin to improve system transaction efficiency.?\n",
      "What is the bitcoin of The SpaceX CEO has in recent months often tweeted in support of Dogecoin, but rarely for Bitcoin.  ?\n",
      "What is the bitcoin of In a recent tweet, Musk put out a statement from Tesla that it was concerned about the rapidly increasing use of fossil fuels for Bitcoin (price in India) mining and transaction, and hence was suspending vehicle purchases using the cryptocurrency.?\n",
      "What is the bitcoin of It triggered a downward spiral for Bitcoin value but the cryptocurrency has stabilised since.  ?\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to generate questions from a keyword\n",
    "def generate_questions(keyword, text):\n",
    "    # Create a SpaCy Doc object for the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Generate questions using a question template\n",
    "    questions = []\n",
    "    question_template = \"What is the {0} of {1}?\"\n",
    "    for sent in doc.sents:\n",
    "        if keyword.lower() in sent.text.lower():\n",
    "            question = question_template.format(keyword, sent.text)\n",
    "            questions.append(question)\n",
    "\n",
    "    return questions\n",
    "\n",
    "# Generate questions for each keyword in the list\n",
    "generated_questions = []\n",
    "for keyword in keywords_summary:\n",
    "    questions = generate_questions(keyword, input_text)\n",
    "    generated_questions.extend(questions)\n",
    "\n",
    "# Print the generated questions\n",
    "for question in generated_questions:\n",
    "    print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8a26101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AJITESH\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AJITESH\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bob greene: the nile is 4,160 miles long — the world’\n",
      "False\n",
      "hundreds - The Greek historian knew what he was talking about. the Nile is\n",
      "a spokesman for the u.s. government said the Nile is\n",
      "aaron miller: the Nile is 4,160 miles long — the world\n",
      "a mediterranean era has produced questions about the mediterran\n",
      "a spokesman for the u.s. government said the river was a\n",
      "nile\n",
      "longest - The Greek historian knew what he was talking about.\n",
      "Nile River is 4,160 miles long — the world’s longest river. it\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Generate questions for each keyword\n",
    "generated_questions = []\n",
    "\n",
    "# Generate questions using T5\n",
    "for keyword in keywords_summary:\n",
    "    input_prompt = f\"generate questions: {keyword} - {input_text}\"\n",
    "    input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids)\n",
    "    generated_question = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_questions.append(generated_question)\n",
    "\n",
    "# Print the generated questions\n",
    "for question in generated_questions:\n",
    "    print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade40d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08b7e6fb",
   "metadata": {},
   "source": [
    "# Distractor generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e6b895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 7.440749168395996 secs.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\AJITESH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AJITESH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AJITESH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AJITESH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\AJITESH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Getting word sense to obtain the best MCQ options with WordNet...\n",
      "Word Sense: cricket_bat.n.01\n",
      "6. Obtaining relative options from WordNet...\n",
      "WordNet Distractors: ['Cricket Ball', 'Cricket Bat', 'Wicket']\n",
      "6. Obtaining relative options from ConceptNet...\n",
      "ConceptNet Distractors: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import random\n",
    "from pywsd.similarity import max_similarity\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from nltk.corpus import wordnet \n",
    "# from find_sentances import extract_sentences\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def wordnet_distractors(syon, word):\n",
    "    print(\"6. Obtaining relative options from WordNet...\")\n",
    "    distractors = []\n",
    "    word = word.lower()\n",
    "    ori_word = word\n",
    "\n",
    "    # Checking if the word is more than one word, then make it one word with _\n",
    "    if len(word.split()) > 0:\n",
    "        word = word.replace(\" \", \"_\")\n",
    "\n",
    "    hypersyon = syon.hypernyms()\n",
    "    if len(hypersyon) == 0:\n",
    "        return distractors\n",
    "    for i in hypersyon[0].hyponyms():\n",
    "        name = i.lemmas()[0].name()\n",
    "\n",
    "        if name == ori_word:\n",
    "            continue\n",
    "        name = name.replace(\"_\", \" \")\n",
    "        name = \" \".join(i.capitalize() for i in name.split())\n",
    "        if name is not None and name not in distractors:\n",
    "            distractors.append(name)\n",
    "    return distractors\n",
    "\n",
    "\n",
    "def conceptnet_distractors(word):\n",
    "    print(\"6. Obtaining relative options from ConceptNet...\")\n",
    "    word = word.lower()\n",
    "    orig_word = word\n",
    "    if len(word.split()) > 0:\n",
    "        word = word.replace(\" \", \"_\")\n",
    "    distractor_list = []\n",
    "    url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\" % (word, word)\n",
    "    obj = requests.get(url).json()\n",
    "\n",
    "    for edge in obj['edges']:\n",
    "        link = edge['end']['term']\n",
    "\n",
    "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\" % (link, link)\n",
    "        obj2 = requests.get(url2).json()\n",
    "        for edge in obj2['edges']:\n",
    "            word2 = edge['start']['label']\n",
    "            if word2 not in distractor_list and orig_word.lower() not in word2.lower():\n",
    "                distractor_list.append(word2)\n",
    "\n",
    "    return distractor_list\n",
    "\n",
    "\n",
    "def word_sense(sentence, keyword):\n",
    "    print(\"5. Getting word sense to obtain the best MCQ options with WordNet...\")\n",
    "    word = keyword.lower()\n",
    "    if len(word.split()) > 0:\n",
    "        word = word.replace(\" \", \"_\")\n",
    "\n",
    "    syn_sets = wordnet.synsets(word, 'n')\n",
    "\n",
    "    if syn_sets:\n",
    "        try:\n",
    "            wup = max_similarity(sentence, word, 'wup', pos='n')\n",
    "            adapted_lesk_output = adapted_lesk(sentence, word, pos='n')\n",
    "            lowest_index = min(syn_sets.index(wup), syn_sets.index(adapted_lesk_output))\n",
    "            return syn_sets[lowest_index]\n",
    "\n",
    "        except:\n",
    "            return syn_sets[0]\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "keyword = \"Bat\"\n",
    "sentence = \"Bat and Ball\"\n",
    "\n",
    "# Obtain word sense\n",
    "synset = word_sense(sentence, keyword)\n",
    "if synset is not None:\n",
    "    print(\"Word Sense:\", synset.name())\n",
    "\n",
    "    # Obtain WordNet distractors\n",
    "    wn_distractors = wordnet_distractors(synset, keyword)\n",
    "    print(\"WordNet Distractors:\", wn_distractors)\n",
    "\n",
    "# Obtain ConceptNet distractors\n",
    "cn_distractors = conceptnet_distractors(keyword)\n",
    "print(\"ConceptNet Distractors:\", cn_distractors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60ac80eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################################################\n",
      "NOTE::::::::  Since the algorithm might have errors along the way, wrong answer choices generated might not be correct for some questions. \n",
      "#############################################################################\n",
      "\n",
      "\n",
      "1) It begins near the equator in Africa and flows  _______  to the Mediterranean Sea\n",
      "\t a )   New England\n",
      "\t b )   Florida\n",
      "\t c )   Kansas\n",
      "\t d )   North\n",
      "\n",
      "More options:  ['Montana', 'Twin', 'Alabama', 'Yosemite', 'Connecticut', 'Mid-Atlantic states', 'New Mexico'] \n",
      "\n",
      "\n",
      "2) It begins near the  _______  in Africa and flows north to the Mediterranean Sea\n",
      "\t a )   Horizon\n",
      "\t b )   Ecliptic\n",
      "\t c )   Equator\n",
      "\t d )   Celestial Equator\n",
      "\n",
      "More options:  ['Hour Circle', 'Meridian', 'Vertical Circle'] \n",
      "\n",
      "\n",
      "3) The Nile River fed Egyptian civilization for  _______  of years.\n",
      "\t a )   Hundreds\n",
      "\t b )   Aleph-null\n",
      "\t c )   Billion\n",
      "\t d )   Crore\n",
      "\n",
      "More options:  ['Eighteen', 'Eighty', 'Eleven', 'Fifteen', 'Fifty', 'Five Hundred', 'Forty', 'Fourteen', 'Great Gross', 'Gross', 'Hundred', 'Hundred Thousand', 'Long Hundred', 'Million', 'Nineteen', 'Ninety'] \n",
      "\n",
      "\n",
      "4) The Nile River fed Egyptian civilization for hundreds of  _______ .\n",
      "\t a )   Air Alert\n",
      "\t b )   Years\n",
      "\t c )   Bimillennium\n",
      "\t d )   Bimester\n",
      "\n",
      "More options:  ['Bout', 'Bronze Age', 'Calendar Day', 'Calendar Month', 'Century', 'Clotting Time', 'Dawn', 'Day', 'Decade', 'Dog Days', 'Downtime', 'Drought', 'Duration', 'Elapsed Time', 'Enlistment', 'Era'] \n",
      "\n",
      "\n",
      "5) The Longest  _______  the Nile is 4,160 miles long — the world’s longest  _______ .\n",
      "\t a )   Headstream\n",
      "\t b )   River\n",
      "\t c )   Branch\n",
      "\t d )   Brook\n",
      "\n",
      "More options:  ['Rivulet', 'Tidal River'] \n",
      "\n",
      "\n",
      "6) The Longest River the  _______  is 4,160 miles long — the world’s longest river.\n",
      "\t a )   Nile\n",
      "\t b )   Buganda\n",
      "\t c )   Gulu\n",
      "\t d )   Entebbe\n",
      "\n",
      "More options:  ['Jinja', 'Lake Edward', 'kayunga', 'gulu', 'entebbe', 'Port Sudan', 'Omdurman', 'Darfur', 'Libyan Desert', 'Kordofan', 'Khartoum', 'Nubian Desert', 'Nyala', 'Aswan High Dam', 'Eastern Desert', 'Aswan'] \n",
      "\n",
      "\n",
      "7) The Nile River fed  _______  civilization for hundreds of years.\n",
      "\t a )   Bantu\n",
      "\t b )   Algerian\n",
      "\t c )   Egyptian\n",
      "\t d )   Angolan\n",
      "\n",
      "More options:  ['Basotho', 'Beninese', 'Berber', 'Black African', 'Burundian', 'Cameroonian', 'Carthaginian', 'Chadian', 'Chewa', 'Congolese', 'Djiboutian', 'Egyptian', 'Ethiopian', 'Eurafrican', 'Ewe', 'Fulani'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from pywsd.similarity import max_similarity\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.lesk import cosine_lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Distractors from Wordnet\n",
    "def get_distractors_wordnet(syn,word):\n",
    "    distractors=[]\n",
    "    word= word.lower()\n",
    "    orig_word = word\n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    hypernym = syn.hypernyms()\n",
    "    if len(hypernym) == 0: \n",
    "        return distractors\n",
    "    for item in hypernym[0].hyponyms():\n",
    "        name = item.lemmas()[0].name()\n",
    "        #print (\"name \",name, \" word\",orig_word)\n",
    "        if name == orig_word:\n",
    "            continue\n",
    "        name = name.replace(\"_\",\" \")\n",
    "        name = \" \".join(w.capitalize() for w in name.split())\n",
    "        if name is not None and name not in distractors:\n",
    "            distractors.append(name)\n",
    "    return distractors\n",
    "\n",
    "def get_wordsense(sent,word):\n",
    "    word= word.lower()\n",
    "    \n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    \n",
    "    \n",
    "    synsets = wn.synsets(word,'n')\n",
    "    if synsets:\n",
    "        wup = max_similarity(sent, word, 'wup', pos='n')\n",
    "        adapted_lesk_output =  adapted_lesk(sent, word, pos='n')\n",
    "        lowest_index = min (synsets.index(wup),synsets.index(adapted_lesk_output))\n",
    "        return synsets[lowest_index]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Distractors from http://conceptnet.io/\n",
    "def get_distractors_conceptnet(word):\n",
    "    word = word.lower()\n",
    "    original_word= word\n",
    "    if (len(word.split())>0):\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    distractor_list = [] \n",
    "    url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n",
    "    obj = requests.get(url).json()\n",
    "\n",
    "    for edge in obj['edges']:\n",
    "        link = edge['end']['term'] \n",
    "\n",
    "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "        obj2 = requests.get(url2).json()\n",
    "        for edge in obj2['edges']:\n",
    "            word2 = edge['start']['label']\n",
    "            if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
    "                distractor_list.append(word2)\n",
    "                   \n",
    "    return distractor_list\n",
    "\n",
    "key_distractor_list = {}\n",
    "\n",
    "for keyword in keyword_sentence_mapping:\n",
    "    wordsense = get_wordsense(keyword_sentence_mapping[keyword][0],keyword)\n",
    "    if wordsense:\n",
    "        distractors = get_distractors_wordnet(wordsense,keyword)\n",
    "        if len(distractors) ==0:\n",
    "            distractors = get_distractors_conceptnet(keyword)\n",
    "        if len(distractors) != 0:\n",
    "            key_distractor_list[keyword] = distractors\n",
    "    else:\n",
    "        \n",
    "        distractors = get_distractors_conceptnet(keyword)\n",
    "        if len(distractors) != 0:\n",
    "            key_distractor_list[keyword] = distractors\n",
    "\n",
    "index = 1\n",
    "print (\"#############################################################################\")\n",
    "print (\"NOTE::::::::  Since the algorithm might have errors along the way, wrong answer choices generated might not be correct for some questions. \")\n",
    "print (\"#############################################################################\\n\\n\")\n",
    "for each in key_distractor_list:\n",
    "    sentence = keyword_sentence_mapping[each][0]\n",
    "    pattern = re.compile(each, re.IGNORECASE)\n",
    "    output = pattern.sub( \" _______ \", sentence)\n",
    "    print (\"%s)\"%(index),output)\n",
    "    choices = [each.capitalize()] + key_distractor_list[each]\n",
    "    top4choices = choices[:4]\n",
    "    random.shuffle(top4choices)\n",
    "    optionchoices = ['a','b','c','d']\n",
    "    for idx,choice in enumerate(top4choices):\n",
    "        print (\"\\t\",optionchoices[idx],\")\",\" \",choice)\n",
    "    print (\"\\nMore options: \", choices[4:20],\"\\n\\n\")\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedc73f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
